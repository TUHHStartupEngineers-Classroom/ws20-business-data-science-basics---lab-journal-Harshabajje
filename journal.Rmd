---
title: "Journal (reproducible report)"
author: "Harsha Bajje Thippeswamy"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

Last compiled: `r Sys.Date()`


# Challenge: Data Analysis

## Bike data Analysis using tidyverse

```{r ,c1challenge_1, fig.width=14, fig.height=7}
# business Data Science at TUHH -------------
# 1.0 Load libraries ----
library(readxl)
library(dplyr)
library(stringr)
library(rlang)
library(tidyr)
library(lubridate)
library(ggplot2)
library(writexl)
library(readr)

# Importing Files ----
# A good convention is to use the file name and suffix it with tbl for the tidyverse data structure tibble

bikes_tbl <- read_excel("C:/Users/harsh/Desktop/data_science/DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel("C:/Users/harsh/Desktop/data_science/DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops_tbl <- read_excel("C:/Users/harsh/Desktop/data_science/DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")

# Joining Data ----
left_join(orderlines_tbl, bikes_tbl, by = c("product.id" = "bike.id"))

bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))

bike_orderlines_joined_tbl %>% 
  select(category) %>%
  filter(str_detect(category, "^Mountain")) %>% 
  unique()

bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%
  # Separate category nam
  separate (col= category,into   = c("category.1", "category.2", "category.3"),sep= " - ")%>% 
  # Add the total price (price * quantity) 
  mutate(total.price = price * quantity) %>%
  select(-...1, -gender) %>%select(-ends_with(".id")) %>%
  bind_cols(bike_orderlines_joined_tbl %>% select(order.id)) %>%
  # You can reorder the data by selecting the columns in your desired order.
  select(order.id, contains("order"), contains("model"), contains("category"),
         price, quantity, total.price,
         everything()) %>%
  # Renaming the columns because we actually wanted to underscores instead of the dots
  rename(bikeshop = name) %>%
  set_names(names(.) %>% str_replace_all("\\.", "_"))



# Challenge :1 
# Business Insights ----
# Sales by State ----
sales_by_state_cat_1_tbl <- bike_orderlines_wrangled_tbl %>%
  tidyr::separate(col = location,
                  into = c("city","state"),
                  sep = ",") %>% 

  select(state, total_price) %>%
  
  # Group by and summarize year and main catgegory
  group_by(state) %>%
  summarise(sales = sum(total_price)) %>%
  ungroup() 


sales_by_state_cat_1_tbl %>%
  
  # Set up x, y, fill
  ggplot(aes(x = state, y = sales)) +
  
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
  
  # Formatting
  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. 
  # Again, we have to adjust it for euro value
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = "ϵ")) +
  labs(
    title = "Revenue by year and main category",
    subtitle = "Each product category has an upward trend",
    fill = "Main category" # Changes the legend name
  )

# Challenge :2

sales_by_year_state_cat_1_tbl <- bike_orderlines_wrangled_tbl %>%
  tidyr::separate(col = location,
                  into = c("city","state"),
                  sep = ",") %>% 
  
  # Select columns and add a year
  select(state,order_date, total_price) %>%
  mutate(year = year(order_date)) %>% 
  
  # Group by and summarize year and main category
  group_by(year,state) %>%
  summarise(sales = sum(total_price)) %>%
  ungroup() 

# Step 2 - Visualize
sales_by_year_state_cat_1_tbl %>%
  
  # Set up x, y, fill
  ggplot(aes(x = year, y = sales, fill = state )) +
  
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
  
  facet_wrap(~ state)+
  
  # Formatting
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = "ϵ")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  
  labs(
    title = "Revenue by year and main category",
    subtitle = "Each product category has an upward trend",
    fill = "Main category" # Changes the legend name
  )

bike_orderlines_wrangled_tbl %>% 
  write_rds("C:/Users/harsh/Desktop/data_science/DS_101/00_data/01_bike_sales/02_wrangled_data/bike_orderlines.rds")
```
# Challenge : Data Retrieval

## Data Aquisition  through API

This API gives us information of people staying in Space, data provided by NASA

```{r, include=TRUE, results='asis'}
# Load libraries ----
library(httr)
library(jsonlite)
library(tidyverse)

# In this example, I worked with the Open Notify API, which opens up data on various NASA projects.
# Request Data ----

res = GET("http://api.open-notify.org/astros.json")

res

#3.0 Convert JSON to Data Structure ----

data = fromJSON(rawToChar(res$content))

names(data)

data$people
```

```{r, echo = TRUE}

# WEBSCRAPING ----


# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing

# 1.1 COLLECTING DIFFERENT  PRODUCT TYPES ----

url_home          <- "https://www.radon-bikes.de/"
xopen(url_home)


html_home         <- read_html(url_home)
bike_family_tbl <- html_home %>%
  html_nodes(css = ".megamenu__item > a") %>%  
  html_attr('href') %>%  
  discard(.p = ~stringr::str_detect(.x,"wear")) %>%  
  enframe(name = "position", value = "cat_subcat_url") %>%  
  mutate(family_id = str_glue("https://www.radon-bikes.de{cat_subcat_url}bikegrid"))
bike_family_tbl


# 2.0 COLLECT BIKE DATA ----

bike_category_url <- bike_family_tbl$family_id[1]
xopen(bike_category_url)


html_bike_category  <- read_html(bike_category_url)

bike_name_tbl        <- html_bike_category %>%
  html_nodes(css = ".m-bikegrid__info .a-heading--small") %>%
  html_text() %>%
  enframe(name = "position", value = "name")
bike_name_tbl 

bike_price_tbl <- html_bike_category %>%
  html_nodes(css = ".m-bikegrid__price.currency_eur .m-bikegrid__price--active") %>%  
  html_text() %>% 
  enframe(name = "position", value = "price")
bike_price_tbl

model_price_tbl <- left_join(bike_name_tbl, bike_price_tbl)%>% 
  select(name, price)
model_price_tbl
```
# Data wrangling
```
# Data Wrangling
## Challange I
```{r c3challenge_1, fig.width=14, fig.height=7}
library(vroom)
library(tidyverse)
library(readxl)
library(lubridate)
library("writexl")
library(stringr)
library(dplyr)
library(tidyr)
col_types <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)
#load stuff
assignee_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling/assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
patent_assignee_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling/patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
#gen table
Patent_Dominance_tbl <- tibble()
#filter us companys
assignee_tbl <- assignee_tbl %>%
  filter(type == 2)
#assamble
Patent_Dominance_tbl <- assignee_tbl %>%
  left_join(patent_assignee_tbl, by = c("id" = "assignee_id")) %>%
  group_by(organization) %>%
  summarise(count = n())%>%
  arrange(desc(count))%>%
  slice(1:10)
#show
glimpse(Patent_Dominance_tbl)
```
## Challange II
```{r c3challenge_2, fig.width=14, fig.height=7}
library(vroom)
library(tidyverse)
library(readxl)
library(lubridate)
library("writexl")
library(stringr)
library(dplyr)
library(tidyr)
col_types <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)
#load stuff
assignee_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling//assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
patent_assignee_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling/patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
patent_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling/patent.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
#genearte stuff
Recent_patent_acitivity_tbl <- tibble()
#filter us companys
assignee_tbl <- assignee_tbl %>%
  filter(type == 2)
#assamble it
Recent_patent_acitivity_tbl <- assignee_tbl %>%
  left_join(patent_assignee_tbl, by = c("id" = "assignee_id")) %>%
  left_join(patent_tbl, by = c("patent_id" = "id")) %>%
  mutate(year = year(date)) %>%
  filter(year == 2019)%>%
  group_by(organization) %>%
  summarise(count = n())%>%
  arrange(desc(count))%>%
  slice(1:10)
#show it
glimpse(Recent_patent_acitivity_tbl)
```
## Challange III
```{r c3challenge_3, fig.width=14, fig.height=7}
library(vroom)
library(tidyverse)
library(stringr)
library(dplyr)
col_types <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)
#load stuff
assignee_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling/assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
patent_assignee_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling/patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
uspc_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling/uspc.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
mainclass_current_tbl <- vroom(
  file       = "C:/Users/harsh/Desktop/data_science/DS_101/00_data/02_data_wrangling/mainclass_current.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
#generate tibbles
getTenTable <- tibble()
wholeTable <- tibble()
filterdTable<- tibble()
filterdTable_summeried<- tibble()
#filter companys
assignee_tbl <- assignee_tbl %>%
  filter(type == 2 | type == 3)
#get the 10 biggest companys
getTenTable <- assignee_tbl %>%
  left_join(patent_assignee_tbl, by = c("id" = "assignee_id")) %>%
  left_join(uspc_tbl, by = c("patent_id" = "patent_id")) %>%
  #filter(!is.na(mainclass_id))%>%
  group_by(organization) %>%
  summarise(count = n())%>%
  arrange(desc(count)) %>%
  ungroup() %>%
  slice(1:10)
topTen <- getTenTable[,c("organization")]
#put together everything
wholeTable <- assignee_tbl %>%
  left_join(patent_assignee_tbl, by = c("id" = "assignee_id")) %>%
  left_join(uspc_tbl, by = c("patent_id" = "patent_id"))
#filter the big 10 
filterdTable <- subset(wholeTable, wholeTable$organization %in% c(topTen[1,1],topTen[2,1],topTen[3,1],topTen[4,1],topTen[5,1],topTen[6,1],topTen[7,1],topTen[8,1],topTen[9,1],topTen[10,1]))
#assable the summary
filterdTable_summeried <- filterdTable %>%
  filter(!is.na(mainclass_id))%>%
  group_by(mainclass_id) %>%
  summarise(count = n())%>%
  arrange(desc(count)) %>%
  ungroup() %>%
  left_join(mainclass_current_tbl, by = c("mainclass_id" = "id")) %>%
  slice(1:10)
#show 
glimpse(filterdTable_summeried)
```
# Challenge: Data Visualization

## Visualize the growth of Covid-19 cases
```{r, echo = TRUE}
# Load libraries ----
library(data.table)
library(tidyverse)
library(lubridate)
library(ggthemes)
library(scales)

# Import ----
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

# Data wrangling ----
covid_data_tbl <- covid_data_tbl %>% 
  mutate(dateRep = as.Date(dateRep, "%d/%m/%Y")) %>%
  arrange(dateRep)



covid_2020_tbl <- covid_data_tbl %>%
  # Select relevant columns
  select(countriesAndTerritories, dateRep, cases, year, month) %>% 
  filter(year == 2020) %>% 
  # Filter countries, add formatted date, gorup and arrange
  filter(countriesAndTerritories %in% c("Germany", "United_Kingdom", "Spain", "France", "United_States_of_America") ) %>%
  arrange(countriesAndTerritories) %>% 
  group_by(countriesAndTerritories) %>%
  # Add cumulative sum of cases
  mutate(cumulative_sum = cumsum(cases)) %>% 
  ungroup()

# Visualization ----
covid_2020_tbl %>%    
  ggplot(aes(dateRep, cumulative_sum, color = countriesAndTerritories)) +
  geom_line(size = 1) +
  
  scale_x_date(date_breaks = "1 month", date_labels = "%B" )  + scale_colour_manual(values = c("#cb75e0", "#75e0d2", "#7ae075", "#dee075", "#e07575")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_y_continuous(labels =  number_format(scale = 1e-6, suffix = " M")) +
  theme_minimal() +
  labs(
    title = "COVID-19 Confirmed cases Worldwide",
    subtitle = "USA the most affected country",
    tag = "Challenge 1",
    x = "Year 2020",
    y = "Cummulative Cases",
    color = "Countries" # Legend text
  ) +
  
  
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold"),
    plot.tag = element_text(face = "bold"),
    plot.tag.position =  "bottom"
  ) +
  
  geom_label( label = max(covid_2020_tbl %>% select(cumulative_sum)),
              vjust = 0.5,
              hjust = 1.5,
              size  = 3,
              data  = covid_2020_tbl %>% 
                filter(countriesAndTerritories %in% c("United_States_of_America")) %>% 
                filter(dateRep == max(covid_2020_tbl$dateRep))
  ) 


```
## Mortality Map
```{r, echo = TRUE}

# Load libraries ----
library(tidyverse)
library(maps)

# Import ----
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")
world <- map_data("world")

# Data wrangling ----
covid_tbl <- covid_data_tbl %>%
  
  # Select relevant columns
  select(deaths, popData2019, countriesAndTerritories) %>%
  
  # Calculate total deaths
  group_by(countriesAndTerritories) %>%
  mutate(total_deaths = sum(deaths)) %>%
  ungroup() %>%
  
  select(popData2019, countriesAndTerritories, total_deaths) %>%
  unique() %>%
  
  # Add deaths with respect to total population
  mutate(deaths_percent = total_deaths/popData2019) %>%
  select(countriesAndTerritories, deaths_percent) %>% 
  
  # Handle differences in country names
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
    
  ))
# Merge data
combined_data <- merge(x = covid_tbl, y = world, 
                         by.x    = "countriesAndTerritories", 
                         by.y    = "region",
                         all.x = FALSE, 
                         all.y = FALSE)
# 4.0 Visualization ----
ggplot(combined_data) +
  
  #Data representation & Legend 
  scale_fill_gradient(low = "red", high = "black", name = "Mortality Rate",
                      n.breaks = 4) +
  
  # Apply base layer for countries without data
  geom_map(dat=world, map = world,
           aes(map_id=region), fill="grey", color="white") +
  
  
  # Apply main map data layer
  geom_map(aes(fill = deaths_percent, map_id = countriesAndTerritories), map = world,
           color = "#ffffff", size=0.000001) +
  
  expand_limits(x = world$long, y = world$lat) +
  
  labs(
    title = "Confirmed COVID-19 deaths relative to the size of the population",
    subtitle = "More than 1.2 Million confirmed deaths worldwide",
    caption = "Date: 06-12-2020",
    x = "",
    y = ""
  ) + 
  
  # Remove Axis labels (long & lat)
  theme(axis.text.x=element_blank()) +
  theme(axis.text.y=element_blank())
```
